{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96542315",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing Demonstration\n",
    "#### Over Netflix Movies and TV Shows Dataset from Kaggle\n",
    "\n",
    "\n",
    "Notes:\n",
    "- After running this notebook you should find `cleaned_netflix_titles.csv` in the repository root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074f358",
   "metadata": {},
   "source": [
    "### Step 1 — Install dependencies\n",
    "This cell installs required Python packages from `requirements.txt`. Run this once after activating your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a7646",
   "metadata": {},
   "source": [
    "### Step 2 — Imports\n",
    "Imports core libraries used for cleaning (`pandas`, `numpy`). Add other imports here if you extend the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8696aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8f478",
   "metadata": {},
   "source": [
    "### Step 3 — Load dataset\n",
    "Reads `netflix_titles.csv` into a pandas DataFrame and records the initial row count for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('netflix_titles.csv')\n",
    "initial_count = df.shape[0]\n",
    "print(df.shape)\n",
    "print(initial_count)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44955ff",
   "metadata": {},
   "source": [
    "### Step 4 — Remove duplicates\n",
    "Calls `df.drop_duplicates()` to remove exact duplicate rows to avoid redundant records before further cleaning and shows sample rows afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a764a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e3318",
   "metadata": {},
   "source": [
    "### Step 5 — Report duplicate removal\n",
    "Computes how many rows were removed by deduplication and prints the new shape and null counts to inspect missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113ef423",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_after_drop_duplicates = df.shape[0]\n",
    "count_drop = initial_count - count_after_drop_duplicates\n",
    "print(\"Number of rows dropped: \", count_drop)\n",
    "print(df.shape)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a4845",
   "metadata": {},
   "source": [
    "### Step 6 — Drop rows missing key fields\n",
    "Drops rows where `title` or `type` are missing — these are considered essential fields for the dataset and resets the DataFrame index. Prints how many rows were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['title', 'type']).reset_index(drop=True) #More logical\n",
    "# df = df.dropna().reset_index(drop=True) #Not logical since dropping other coloumns than title and type makes no sense\n",
    "print(df.shape)\n",
    "count_after_dropping_na = df.shape[0]\n",
    "count_drop_na = count_after_drop_duplicates - count_after_dropping_na\n",
    "print(\"Number of rows dropped (NA): \", count_drop_na)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77fd70",
   "metadata": {},
   "source": [
    "### Step 7 — Normalize text columns\n",
    "Strips whitespace, converts to lowercase, and converts 'nan' strings back to a proper `np.nan` for selected text columns to standardize values for later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23990bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['country', 'type', 'director', 'cast', 'listed_in']\n",
    "for col in text_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip().replace('nan', np.nan)\n",
    "        df[col] = df[col].where(df[col].isna(), df[col].str.lower())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a4a62",
   "metadata": {},
   "source": [
    "### Step 8 — Parse & format dates, convert types\n",
    "Parses `date_added` with a flexible parser into a consistent `dd-mm-yyyy` string where possible, converts `release_year` to integer type `Int64`, and casts `rating` and `type` to categorical dtype where present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'date_added' in df.columns:\n",
    "    from dateutil import parser\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    def parse_date_flexible(date_str):\n",
    "        if pd.isna(date_str):\n",
    "            return None\n",
    "        try:\n",
    "            return parser.parse(str(date_str))\n",
    "        except:\n",
    "            try:\n",
    "                return pd.to_datetime(date_str, format='%B %d, %Y')\n",
    "            except:\n",
    "                return None\n",
    "    \n",
    "    df['date_added'] = df['date_added'].apply(parse_date_flexible)\n",
    "    \n",
    "    df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "\n",
    "if 'release_year' in df.columns:\n",
    "    df['release_year'] = pd.to_numeric(df['release_year'], errors='coerce').astype('Int64')\n",
    "    \n",
    "if 'rating' in df.columns:\n",
    "    df['rating'] = df['rating'].astype('category')\n",
    "    \n",
    "if 'type' in df.columns:\n",
    "    df['type'] = df['type'].astype('category')\n",
    "    \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62df2db",
   "metadata": {},
   "source": [
    "### Step 9 — Normalize column names\n",
    "Converts column headers to lowercase and replaces spaces with underscores for consistency in downstream code (e.g., `Date Added` -> `date_added`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052a0ec",
   "metadata": {},
   "source": [
    "### Step 10 — Final checks & save\n",
    "Prints final diagnostics (shape, null counts, dtypes) and writes the cleaned dataset to `cleaned_netflix_titles.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final shape:', df.shape)\n",
    "print('\\nNull counts after cleaning:\\n', df.isnull().sum(), '\\n')\n",
    "print(df.dtypes)\n",
    "df.to_csv('cleaned_netflix_titles.csv', index=False)\n",
    "print('Saved cleaned dataset to cleaned_netflix_titles.csv')\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".netflix_env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
